

1.åŠ è½½æ¨¡å‹å’Œä¿å­˜æ¨¡å‹
model.save_pretrained("model_dir")
torch.save(model_for_cls.state_dict(), "model_dir/pytorch_model.bin")
åŒºåˆ«åªæ˜¯ä¸€ä¸ªæ˜¯ä¿å­˜æ¨¡å‹ç»“æ„å’Œæƒé‡ï¼Œä¸€ä¸ªæ˜¯åªä¿å­˜æ¨¡å‹æƒé‡

class MyClassifier(nn.Module):  # â† ä»…ç»§æ‰¿ torch.nn.Module
    def __init__(self, model_name="Qwen/Qwen1.5-0.5B", num_labels=2):
        super().__init__()
        self.backbone = AutoModel.from_pretrained(model_name, trust_remote_code=True)
        self.classifier = nn.Linear(self.backbone.config.hidden_size, num_labels)
å¦‚æœä½ çš„æ¨¡å‹ç±»æ²¡æœ‰ç»§æ‰¿ PreTrainedModelï¼ˆæˆ–å…¶å­ç±»ï¼‰ï¼Œä½ å°±ä¸èƒ½ç›´æ¥è°ƒç”¨ model.save_pretrained()ã€‚

æ–¹æ³• 1ï¼šä¿å­˜ state_dictï¼ˆæ¨èï¼‰
ğŸ”§ ä¿å­˜ï¼š
torch.save(model.state_dict(), "my_model.pth")
ğŸ” åŠ è½½ï¼š
# 1. å¿…é¡»å…ˆé‡æ–°å®šä¹‰å®Œå…¨ç›¸åŒçš„æ¨¡å‹ç»“æ„
model = MyClassifier(model_args)
# 2. åŠ è½½æƒé‡
state_dict = torch.load("my_model.pth", map_location="cpu")  # æˆ– "cuda"
model.load_state_dict(state_dict)

# 3. åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ï¼ˆå¦‚ç”¨äºæ¨ç†ï¼‰
model.eval()

æ–¹æ³• 2ï¼šæ‰‹åŠ¨ä¿å­˜ state_dict + é…ç½®æ–‡ä»¶ï¼ˆå¢å¼ºç‰ˆæ–¹æ³• 1ï¼‰
ä¸ºäº†è§£å†³â€œå¿˜è®° num_labelsâ€çš„é—®é¢˜ï¼Œå¯é¢å¤–ä¿å­˜é…ç½®ã€‚

ğŸ”§ ä¿å­˜ï¼š
import json
# ä¿å­˜æƒé‡
torch.save(model.state_dict(), "pytorch_model.bin")

# ä¿å­˜é…ç½®ï¼ˆæ¨¡æ‹Ÿ HF çš„ config.jsonï¼‰
config = {
    "model_name": "Qwen/Qwen1.5-0.5B",
    "num_labels": 5,
    "hidden_size": model.backbone.config.hidden_size
}
with open("model_config.json", "w") as f:
    json.dump(config, f, indent=2)
ğŸ” åŠ è½½ï¼š
Python
ç¼–è¾‘
# 1. è¯»å–é…ç½®
with open("model_config.json") as f:
    config = json.load(f)

# 2. é‡å»ºæ¨¡å‹
model = MyClassifier(
    model_name=config["model_name"],
    num_labels=config["num_labels"]
)

# 3. åŠ è½½æƒé‡
state_dict = torch.load("pytorch_model.bin", map_location="cpu")
model.load_state_dict(state_dict)
model.eval()

--------------------------------------------------------------------------------------
                                æ¨¡å‹å·²æ­£ç¡®ç»§æ‰¿ HF ç±»
from transformers import PreTrainedModel, PretrainedConfig, AutoModel
import torch.nn as nn

# 1. å®šä¹‰é…ç½®ç±»ï¼ˆå¿…é¡»ï¼ï¼‰
class MyCustomConfig(PretrainedConfig):
    model_type = "my_qwen_classifier"  # â† å…³é”®æ ‡è¯†ç¬¦ï¼

    def __init__(self, num_labels=2, backbone_name="Qwen/Qwen1.5-0.5B", **kwargs):
        super().__init__(**kwargs)
        self.num_labels = num_labels
        self.backbone_name = backbone_name

# 2. å®šä¹‰æ¨¡å‹ç±»ï¼ˆç»§æ‰¿ PreTrainedModelï¼‰
class MyQwenClassifier(PreTrainedModel):
    config_class = MyCustomConfig  # â† ç»‘å®š config

    def __init__(self, config):
        super().__init__(config)  # â† è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        self.backbone = AutoModel.from_config(config, trust_remote_code=True)
        self.classifier = nn.Linear(self.backbone.config.hidden_size, config.num_labels)
        self.post_init()  # æ¨èï¼šå¤„ç†æƒé‡åˆå§‹åŒ–ç­‰

    def forward(self, input_ids, attention_mask=None):
        outputs = self.backbone(input_ids, attention_mask=attention_mask)
        logits = self.classifier(outputs.last_hidden_state[:, 0])
        return {"logits": logits}


 ä¿å­˜æ–¹æ³•ï¼ˆæ¨èï¼‰
âœ… æ ‡å‡†æ–¹å¼ï¼šsave_pretrained()
Python
ç¼–è¾‘
config = MyCustomConfig(num_labels=5)
model = MyQwenClassifier(config)

# å¾®è°ƒåä¿å­˜
model.save_pretrained("./my_finetuned_model")
tokenizer.save_pretrained("./my_finetuned_model")  # å¦‚æœæœ‰ tokenizer


 åŠ è½½æ–¹æ³•ï¼ˆæ ‡å‡†ä¸”ç®€å•ï¼‰
âœ… æ–¹å¼ 1ï¼šç›´æ¥ç”¨ä½ çš„ç±»åŠ è½½ï¼ˆæœ€å¸¸ç”¨ï¼‰
from your_module import MyQwenClassifier  # ç¡®ä¿ç±»å¯è§

model = MyQwenClassifier.from_pretrained(
    "./my_finetuned_model",
    trust_remote_code=True  # å¿…é¡»ï¼å› ä¸ºç”¨äº†è‡ªå®šä¹‰æ¨¡å‹
)
2.åŠ è½½tokenizer
# å‡è®¾ä½ æœ‰ä¸€ä¸ª tokenizerï¼ˆæ¥è‡ªåŸå§‹æ¨¡å‹ï¼‰
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen1.5-0.5B", trust_remote_code=True)
# ä¿å­˜åˆ°ä½ çš„æ¨¡å‹ç›®å½•ï¼ˆå³ä½¿æ¨¡å‹æ˜¯è‡ªå®šä¹‰çš„ï¼‰
tokenizer.save_pretrained("./my_custom_model_dir")
åˆ†è¯å™¨è®¾ç½®ï¼štokenizer.padding_side
tokenizer.pad_token

3.æŸå¤±å‡½æ•°
æ‰¾ç°æˆçš„åº“ï¼Œä¸åŒç±»å‹çš„ä»»åŠ¡æœ‰ä¸åŒæŸå¤±å‡½æ•°ä»¬ï¼Œå¯¹äºæ•°æ®é›†å½¢å¼è¦æ±‚å·®åˆ«å¾ˆå¤§

4.æ•°æ®é›†æ„å»º
éœ€è¦è‡ªå®šä¹‰ä¸€ä¸ªæ•°æ®é›†ç±»ï¼Œç»§æ‰¿torch.utils.data.Datasetï¼Œå®ç°__len__å’Œ__getitem__æ–¹æ³•ã€‚
é‡ç‚¹æ˜¯__getitem__æ–¹æ³•éœ€è¦è¿”å›æŸå¤±å‡½æ•°å’Œæ¨¡å‹è¾“å…¥éƒ½èƒ½æ»¡è¶³çš„å½¢å¼



5.è®­ç»ƒ
åŠ è½½åˆ†è¯å™¨ï¼Œæ¨¡å‹ï¼Œæ•°æ®é›†æ”¾å…¥æ¡†æ¶çš„è®­ç»ƒå™¨ï¼Œè®­ç»ƒç»“æŸåä¿å­˜æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶ï¼Œåˆ†è¯å™¨

å¯¹äºæŸå¤±å‡½æ•°
Hugging Face çš„Trainerç”Ÿæ€
å¿…é¡»åœ¨ forward ä¸­è¿”å› lossï¼ˆå½“ä¼ å…¥ labels æ—¶ï¼‰

è¿™æ˜¯ Trainer èƒ½è‡ªåŠ¨è®­ç»ƒçš„å‰æ
Sentence-Transformersç”Ÿæ€ã€
é’ˆå¯¹åµŒå…¥æ¨¡å‹
æ ¸å¿ƒä¼˜åŠ¿å°±æ˜¯ï¼šæ”¯æŒçµæ´»æ›´æ¢æŸå¤±å‡½æ•°

6.æ¨ç†
åŠ è½½å®Œæ¨¡å‹åï¼Œä½¿ç”¨model.eval()åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ï¼Œåˆ†è¯å™¨å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œç¼–ç ï¼Œå°†ç¼–ç åçš„å¼ é‡ä¼ å…¥æ¨¡å‹ï¼Œæ¨¡å‹è¾“å‡ºforwardçš„ç»“æœ


